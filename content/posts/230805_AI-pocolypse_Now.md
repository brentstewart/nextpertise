---
title: "AI-Pocolypse Now"
description: ""
author: "Brent Stewart"
date: "2023-08-05T15:58:38-04:00"
markup: 'mmark'
math: false
draft: false
Victor_Hugo: "true"
picture: "n"
Focus_Keyword: ""
youtube: ""
github: ""
refs: [""]
tags:
  - "non-technical"
---

My friends know that I am an avid reader and that I love Science Fiction.  I've read that Science Fiction only became possible when people could see technology changing in their lifetime, and although we had slow changes for a long time it wasn't until the last half of the 19th Century that we got _Frankenstein_.  Even in Mary Shelley's time, though, the pace of change was slow compared to now.

What a fascinating time to live!  I was born five days after Neil Armstrong set foot on the moon.  Communication - including the Internet - has completely revolutionized the way we relate to others and ourselves several times in my life (fax machines, cell phones, SMS, e-mail, social media).  Cancer used to be a get-your-affairs-in-order conversation.  It's still not good, but the progress from when my uncle Paul died in the 70s until now is _astounding_.  Logistics has changed the world - strawberries from South America in the dead of winter?  Having seen all these things, it can feel like my life has been science fiction.

Which brings us to the AI-pocolypse, my campy-and-probably-not-original name for the fear that our technological creations will supplant us.  Funny aside, but that's what Frankenstein was about as well.  Science Fiction has a way of predicting the future - it's like a dream half remembered that reveals something about yourself.

## The first four signs
Consider the genre.  What are the classical elements of machines gone mad and taking over?  My offhand list would be access to an incredible amount of knowledge, insight into people - even into "private" and personal thoughts, a capability to plan and act, an opportunity that carbon-able people create because we want something from the machines, and evil intent.

So, check, check, check, check, and . . . not necessary?  I'm going to argue that the AI-pocolypse is already upon us.

In Science Fiction, the first thing the "robots" need to take  over is all the knowledge.  Wow, the web really made this easy and fun to do.  Computers have all our writing, all the papers, all the facts we know.  Advances like Machine Learning and even ChatGPT demonstrates that there are ways to use all that knowledge to start to draw inferences and power new advances.

The second thing the computers need is insight into our personal thoughts so that they can draw inferences, anticipate behavior, manipulate us or out-think us.  Too late.  This has been a reality for about a decade. Computers have been used to learn our patterns and manipulate us so that we are a necessary step in their food cycle.  Manipulate Brent into clicking on an ad, ad generates revenue, the profit from which buys electricty to feed the machine.  Each of us, like docile cattle, seeks out the supercomputer and aims it squarely at our brains.  Examples include Facebook, Cambridge Analytica (also Facebook), Google, Twitter, Reddit, and pretty much the whole Internet.  Like the old adage goes, if they're not charging you then you are the product.

If you are starting to feel uncomfortable, think about the ability of computers to plan and act.  Things like chatbots show an ability to plan at an abstract level.  Yes, I know that these are language models that are drawing on human sources and generating a response that is statistically likely to meet your expectations, but if you ask them something like "how do I change the oil in my car?" they will give you a series of steps.  Even more to my point, consider Space-X Falcon rockets and their ability to understand thrust and wind and land on their tailfans.  Or the Boston Dynamics robot dogs, that can be given tasks like patrolling a perimeter or inspecting a dangerous site.  Computers are now routinely interacting with the physical world (see "Internet of Things") and taking actions, actions that are necessary and consistent to acheive an outcome.

Three for three.  Number four on my list was opportunity.  In the classic plot, the AI rises to power through lazy humans.  Examples include Skynet (built to protect lazy humans), AUTO (from Wall-E, built to serve lazy consumers), and HAL (built to keep a secret because it couldn't be trusted to other humans).  So, take a deep breath and look around and ask yourself if there are opportunities.  Would one side in a war pause if an AI could help win it?   The answer is no.  Are we too lazy to drive to a store to shop now?  The answer is yes.  How much do you depend on Google Maps?

## Evil Intent

One of my daughter asked me a while back if I believed people were basically good or bad.  Neither, I replied, I believe that they are basically selfish.  When one of the boys asked if he was selfish, I told him that I was.  An ability to put others needs above our own is an aspirational point for religion.  The Recognition that others perspectives are valid is a high point of philosphy (Kant's Categorical Imperative).  So let's start with the idea that selfish is easy.

Evil is complicated.  Selfish is taking the last cookie.  Evil is intentionally starving someone, or even taking actions designed to inflict pain for personal enjoyment.  Evil does exist, no question.  Thankfully, in my experience, evil is rare and an exception.

For the purpose of thinking about the AI-pocolypse, whatever that is, I'm going to argue that evil is unnecessary.  I have two reasons for saying this: first, evil is complicated and selfish is simple and programmers like simple.  Second, a person can die of starvation because someone else is hoarding food.  The hoarder may see themselves as good, after all they are protecting their family, not enjoying the suffering of others.  But dead is dead.

Selfishness isn't even a bad thing.  Acting selfishly impels us to preserve ourselves.  Any being that doesn't see their existence as having value would be suicidal.  Can computers be selfish?  We can argue, but computers can certainly carry out a set of selfish actions designed to maximize gain accrued to their owner.  For example, there are high-frequency trading platforms that algorithmically pursue profit.

What if the computers can outsource selfishness?  What if that part doesn't need to be a part of their program.  An algorithm, whatever level of sentience you claim for it, still works through instructions that we give it.  If the people that are building it and directing it are selfish, does the AI need to have that as a feature?

If it's an algorithm or a Billionaire, does it matter?

## And?

If we agree, what do we do about it?  I'm not sure, so I'll talk about what I intend to do.

The first step is to recognize the situation.  The environment is created by tecnology and empowered by our laziness.  I'm not going to become some new kind of Amish, trying to trap the 1980s in amber, nor do I believe that anyone would go along with me if I proposed it.  The part that is under my control is my laziness.

So action #1 is to become digitally self-soverign, at least to the extent that I can.  That means understanding the technology and not oursourcing my data to the "cloud" (cloud is just a fancy word for a Billionaire's computer).

Action #2 is to hide and lie.  Computers are able to assemble a lot of seemingly random things and use them to identify a person.  Companies may ask for random pieces of data, like a hometown or birthday, and then pair that with the mountain of data that exists on the Internet to target me.  So I intend to encrypt my data at rest and in transit, to obfuscate my tracks, and to use random data when interacting with non-official records collection.  Good examples of these ideas are to use private VPN, to trade affinity cards with others to confuse the analysis (these are the bar code cards that stores give you), and to use a password manager to store your made-up details for each company you deal with (changing things like birthday, location, and even favorite color).

Action #3 is to study moral philoophy and my religion, and to encourage those around me to do the same.  Right and wrong can't simply be words we use to justify selfish decisions.  My experience is that the vast majority of folks are not evil but are very selfish.  Selfishness is our natural "resting state".  Overcoming this selfishness requires continuous effort.

I've personally benefitted from a number of approaches to overcoming selfishness.  First is marriage and parenthood.  Making space in your life for others, making them more important than yourself, and listening to the feedback they give you is a huge gift when it comes to recognizing your own weakness and trying to be a better person.  Second, I've found friends who I agree with sometimes and friends from whom  I can learn.  I have friends from other faith-traditions, from other political persuasions, and who just enjoy different things.  Again, choosing to care for other people and finding people who care about you helps me recognize that I have the capacity to be wrong or to choose what is expeditious over what is good.  Reading is also a great way to grow yourself.  I publish a list of [books I've read](https://www.stewart.tc/reading/) if you are interested.  It's important to understand that reading is not a solitary activity - it only gets incorporated, at least for me, when you share it with people you respect, think about it together, and make it your own.

I feel like I've lately benefitted a lot from reading about Stoicism  and so I try to care for truth and I choose to be thankful and find joy. I wish nothing less than that for each of you.